{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfb0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 2018310064 λ¬Έν•™μ¤€\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0e95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a64f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98730235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "num_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4952da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet_5,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06df80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919a2d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "download_path = './CIFAR'\n",
    "dataset = dset.CIFAR10(download_path, transform=transform, train = True, download = True)\n",
    "test_dataset = dset.CIFAR10(download_path, transform=transform, train = False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082dac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [40000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a1d101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c5c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet_5().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a754ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funcion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378e843e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 2.3075, val_loss : 2.1355, val_acc : 32.1500 %, lr : 0.001000\n",
      "epoch : 2, train_loss : 2.2635, val_loss : 2.0522, val_acc : 40.8500 %, lr : 0.001000\n",
      "epoch : 4, train_loss : 2.1424, val_loss : 2.0109, val_acc : 45.0200 %, lr : 0.001000\n",
      "epoch : 6, train_loss : 2.0632, val_loss : 1.9840, val_acc : 47.5400 %, lr : 0.001000\n",
      "epoch : 8, train_loss : 1.8958, val_loss : 1.9587, val_acc : 49.8800 %, lr : 0.001000\n",
      "epoch : 10, train_loss : 1.8868, val_loss : 1.9348, val_acc : 52.1500 %, lr : 0.001000\n",
      "epoch : 12, train_loss : 1.8412, val_loss : 1.9227, val_acc : 53.6500 %, lr : 0.001000\n",
      "epoch : 14, train_loss : 1.7860, val_loss : 1.9035, val_acc : 55.1900 %, lr : 0.001000\n",
      "epoch : 16, train_loss : 1.7895, val_loss : 1.8981, val_acc : 55.8400 %, lr : 0.001000\n",
      "epoch : 18, train_loss : 1.8028, val_loss : 1.8906, val_acc : 56.6600 %, lr : 0.001000\n",
      "epoch : 20, train_loss : 1.8194, val_loss : 1.8904, val_acc : 56.5500 %, lr : 0.001000\n",
      "epoch : 22, train_loss : 1.8313, val_loss : 1.8762, val_acc : 58.2100 %, lr : 0.001000\n",
      "epoch : 24, train_loss : 1.8188, val_loss : 1.8784, val_acc : 57.8300 %, lr : 0.001000\n",
      "epoch : 26, train_loss : 1.8467, val_loss : 1.8786, val_acc : 57.9100 %, lr : 0.001000\n",
      "epoch : 28, train_loss : 1.7899, val_loss : 1.8628, val_acc : 59.4500 %, lr : 0.001000\n",
      "epoch : 30, train_loss : 1.7777, val_loss : 1.8666, val_acc : 59.0600 %, lr : 0.001000\n",
      "epoch : 32, train_loss : 1.8203, val_loss : 1.8578, val_acc : 60.1200 %, lr : 0.001000\n",
      "epoch : 34, train_loss : 1.7756, val_loss : 1.8608, val_acc : 59.6000 %, lr : 0.001000\n",
      "epoch : 36, train_loss : 1.8012, val_loss : 1.8546, val_acc : 60.3400 %, lr : 0.001000\n",
      "epoch : 38, train_loss : 1.7800, val_loss : 1.8517, val_acc : 60.4800 %, lr : 0.001000\n",
      "epoch : 40, train_loss : 1.8413, val_loss : 1.8563, val_acc : 60.1700 %, lr : 0.001000\n",
      "epoch : 42, train_loss : 1.8299, val_loss : 1.8643, val_acc : 59.2500 %, lr : 0.001000\n",
      "epoch : 44, train_loss : 1.7880, val_loss : 1.8548, val_acc : 60.2100 %, lr : 0.001000\n",
      "epoch : 46, train_loss : 1.7762, val_loss : 1.8463, val_acc : 61.0600 %, lr : 0.001000\n",
      "epoch : 48, train_loss : 1.8524, val_loss : 1.8539, val_acc : 60.3600 %, lr : 0.001000\n",
      "epoch : 50, train_loss : 1.8212, val_loss : 1.8486, val_acc : 60.7400 %, lr : 0.000750\n",
      "epoch : 52, train_loss : 1.8291, val_loss : 1.8524, val_acc : 60.6400 %, lr : 0.000750\n",
      "epoch : 54, train_loss : 1.7281, val_loss : 1.8438, val_acc : 61.1500 %, lr : 0.000750\n",
      "epoch : 56, train_loss : 1.7561, val_loss : 1.8462, val_acc : 61.0400 %, lr : 0.000750\n",
      "epoch : 58, train_loss : 1.7512, val_loss : 1.8484, val_acc : 60.7800 %, lr : 0.000750\n",
      "epoch : 60, train_loss : 1.7735, val_loss : 1.8453, val_acc : 60.9600 %, lr : 0.000750\n",
      "epoch : 62, train_loss : 1.7859, val_loss : 1.8451, val_acc : 61.0900 %, lr : 0.000750\n",
      "epoch : 64, train_loss : 1.7873, val_loss : 1.8460, val_acc : 61.1300 %, lr : 0.000750\n",
      "epoch : 66, train_loss : 1.7945, val_loss : 1.8478, val_acc : 61.0500 %, lr : 0.000750\n",
      "epoch : 68, train_loss : 1.7749, val_loss : 1.8469, val_acc : 60.9900 %, lr : 0.000750\n",
      "epoch : 70, train_loss : 1.7389, val_loss : 1.8471, val_acc : 61.0300 %, lr : 0.000750\n",
      "epoch : 72, train_loss : 1.7449, val_loss : 1.8471, val_acc : 60.8400 %, lr : 0.000750\n",
      "epoch : 74, train_loss : 1.7826, val_loss : 1.8514, val_acc : 60.4600 %, lr : 0.000750\n",
      "epoch : 76, train_loss : 1.7932, val_loss : 1.8476, val_acc : 60.9700 %, lr : 0.000750\n",
      "epoch : 78, train_loss : 1.7698, val_loss : 1.8477, val_acc : 61.0000 %, lr : 0.000750\n",
      "epoch : 80, train_loss : 1.8654, val_loss : 1.8481, val_acc : 61.0300 %, lr : 0.000750\n",
      "epoch : 82, train_loss : 1.8126, val_loss : 1.8467, val_acc : 61.0200 %, lr : 0.000750\n",
      "epoch : 84, train_loss : 1.8461, val_loss : 1.8479, val_acc : 61.0200 %, lr : 0.000750\n",
      "epoch : 86, train_loss : 1.7526, val_loss : 1.8475, val_acc : 60.9800 %, lr : 0.000750\n",
      "epoch : 88, train_loss : 1.8220, val_loss : 1.8505, val_acc : 60.8100 %, lr : 0.000750\n",
      "epoch : 90, train_loss : 1.7884, val_loss : 1.8442, val_acc : 61.3000 %, lr : 0.000750\n",
      "epoch : 92, train_loss : 1.8555, val_loss : 1.8498, val_acc : 60.8100 %, lr : 0.000750\n",
      "epoch : 94, train_loss : 1.8183, val_loss : 1.8516, val_acc : 60.5600 %, lr : 0.000750\n",
      "epoch : 96, train_loss : 1.8081, val_loss : 1.8494, val_acc : 60.8000 %, lr : 0.000750\n",
      "epoch : 98, train_loss : 1.8000, val_loss : 1.8493, val_acc : 60.9000 %, lr : 0.000750\n",
      "epoch : 100, train_loss : 1.8095, val_loss : 1.8481, val_acc : 60.7400 %, lr : 0.000563\n",
      "epoch : 102, train_loss : 1.8356, val_loss : 1.8495, val_acc : 60.9200 %, lr : 0.000563\n",
      "epoch : 104, train_loss : 1.8093, val_loss : 1.8474, val_acc : 60.9600 %, lr : 0.000563\n",
      "epoch : 106, train_loss : 1.8644, val_loss : 1.8482, val_acc : 61.1800 %, lr : 0.000563\n",
      "epoch : 108, train_loss : 1.7933, val_loss : 1.8484, val_acc : 60.9000 %, lr : 0.000563\n",
      "epoch : 110, train_loss : 1.8773, val_loss : 1.8497, val_acc : 60.8600 %, lr : 0.000563\n",
      "epoch : 112, train_loss : 1.8385, val_loss : 1.8491, val_acc : 60.9800 %, lr : 0.000563\n",
      "epoch : 114, train_loss : 1.8880, val_loss : 1.8517, val_acc : 60.8100 %, lr : 0.000563\n",
      "epoch : 116, train_loss : 1.8448, val_loss : 1.8487, val_acc : 60.9500 %, lr : 0.000563\n",
      "epoch : 118, train_loss : 1.8632, val_loss : 1.8476, val_acc : 61.2500 %, lr : 0.000563\n",
      "epoch : 120, train_loss : 1.8323, val_loss : 1.8462, val_acc : 61.1700 %, lr : 0.000563\n",
      "epoch : 122, train_loss : 1.8891, val_loss : 1.8563, val_acc : 60.3900 %, lr : 0.000563\n",
      "epoch : 124, train_loss : 1.8649, val_loss : 1.8479, val_acc : 61.1600 %, lr : 0.000563\n",
      "epoch : 126, train_loss : 1.8268, val_loss : 1.8490, val_acc : 60.9800 %, lr : 0.000563\n",
      "epoch : 128, train_loss : 1.8357, val_loss : 1.8487, val_acc : 60.9300 %, lr : 0.000563\n",
      "epoch : 130, train_loss : 1.8053, val_loss : 1.8489, val_acc : 60.9200 %, lr : 0.000563\n",
      "epoch : 132, train_loss : 1.8200, val_loss : 1.8483, val_acc : 61.0500 %, lr : 0.000563\n",
      "epoch : 134, train_loss : 1.8785, val_loss : 1.8503, val_acc : 60.9900 %, lr : 0.000563\n",
      "epoch : 136, train_loss : 1.8097, val_loss : 1.8494, val_acc : 60.8400 %, lr : 0.000563\n",
      "epoch : 138, train_loss : 1.8321, val_loss : 1.8506, val_acc : 60.7500 %, lr : 0.000563\n",
      "epoch : 140, train_loss : 1.8180, val_loss : 1.8519, val_acc : 60.5900 %, lr : 0.000563\n",
      "epoch : 142, train_loss : 1.8756, val_loss : 1.8526, val_acc : 60.6300 %, lr : 0.000563\n",
      "epoch : 144, train_loss : 1.8754, val_loss : 1.8518, val_acc : 60.6200 %, lr : 0.000563\n",
      "epoch : 146, train_loss : 1.8264, val_loss : 1.8506, val_acc : 60.7600 %, lr : 0.000563\n",
      "epoch : 148, train_loss : 1.8811, val_loss : 1.8498, val_acc : 60.9000 %, lr : 0.000563\n",
      "epoch : 150, train_loss : 1.8256, val_loss : 1.8480, val_acc : 61.0200 %, lr : 0.000422\n",
      "epoch : 152, train_loss : 1.8421, val_loss : 1.8495, val_acc : 60.7300 %, lr : 0.000422\n",
      "epoch : 154, train_loss : 1.8282, val_loss : 1.8486, val_acc : 60.9000 %, lr : 0.000422\n",
      "epoch : 156, train_loss : 1.8163, val_loss : 1.8501, val_acc : 60.8200 %, lr : 0.000422\n",
      "epoch : 158, train_loss : 1.9035, val_loss : 1.8537, val_acc : 60.3800 %, lr : 0.000422\n",
      "epoch : 160, train_loss : 1.8005, val_loss : 1.8474, val_acc : 60.9600 %, lr : 0.000422\n",
      "epoch : 162, train_loss : 1.8262, val_loss : 1.8449, val_acc : 61.4800 %, lr : 0.000422\n",
      "epoch : 164, train_loss : 1.8316, val_loss : 1.8464, val_acc : 61.2500 %, lr : 0.000422\n",
      "epoch : 166, train_loss : 1.8254, val_loss : 1.8493, val_acc : 60.9000 %, lr : 0.000422\n",
      "epoch : 168, train_loss : 1.8508, val_loss : 1.8564, val_acc : 60.3300 %, lr : 0.000422\n",
      "epoch : 170, train_loss : 1.7919, val_loss : 1.8472, val_acc : 61.0600 %, lr : 0.000422\n",
      "epoch : 172, train_loss : 1.7771, val_loss : 1.8448, val_acc : 61.1900 %, lr : 0.000422\n",
      "epoch : 174, train_loss : 1.7952, val_loss : 1.8451, val_acc : 61.3300 %, lr : 0.000422\n",
      "epoch : 176, train_loss : 1.8048, val_loss : 1.8455, val_acc : 61.3200 %, lr : 0.000422\n",
      "epoch : 178, train_loss : 1.8045, val_loss : 1.8452, val_acc : 61.3400 %, lr : 0.000422\n",
      "epoch : 180, train_loss : 1.8575, val_loss : 1.8500, val_acc : 60.8700 %, lr : 0.000422\n",
      "epoch : 182, train_loss : 1.8580, val_loss : 1.8469, val_acc : 61.2400 %, lr : 0.000422\n",
      "epoch : 184, train_loss : 1.8591, val_loss : 1.8464, val_acc : 61.3200 %, lr : 0.000422\n",
      "epoch : 186, train_loss : 1.8421, val_loss : 1.8455, val_acc : 61.4400 %, lr : 0.000422\n",
      "epoch : 188, train_loss : 1.8823, val_loss : 1.8457, val_acc : 61.4400 %, lr : 0.000422\n",
      "epoch : 190, train_loss : 1.8726, val_loss : 1.8518, val_acc : 60.8000 %, lr : 0.000422\n",
      "epoch : 192, train_loss : 1.8023, val_loss : 1.8463, val_acc : 61.0100 %, lr : 0.000422\n",
      "epoch : 194, train_loss : 1.8408, val_loss : 1.8459, val_acc : 61.3600 %, lr : 0.000422\n",
      "epoch : 196, train_loss : 1.8332, val_loss : 1.8452, val_acc : 61.3800 %, lr : 0.000422\n",
      "epoch : 198, train_loss : 1.8690, val_loss : 1.8461, val_acc : 61.3700 %, lr : 0.000422\n",
      "epoch : 200, train_loss : 1.8963, val_loss : 1.8462, val_acc : 61.3500 %, lr : 0.000316\n",
      "epoch : 202, train_loss : 1.8886, val_loss : 1.8452, val_acc : 61.5800 %, lr : 0.000316\n",
      "epoch : 204, train_loss : 1.8900, val_loss : 1.8454, val_acc : 61.5300 %, lr : 0.000316\n",
      "epoch : 206, train_loss : 1.8907, val_loss : 1.8457, val_acc : 61.4500 %, lr : 0.000316\n",
      "epoch : 208, train_loss : 1.8861, val_loss : 1.8455, val_acc : 61.7100 %, lr : 0.000316\n",
      "epoch : 210, train_loss : 1.8536, val_loss : 1.8505, val_acc : 60.9000 %, lr : 0.000316\n",
      "epoch : 212, train_loss : 1.8572, val_loss : 1.8440, val_acc : 61.7000 %, lr : 0.000316\n",
      "epoch : 214, train_loss : 1.8555, val_loss : 1.8442, val_acc : 61.6100 %, lr : 0.000316\n",
      "epoch : 216, train_loss : 1.8669, val_loss : 1.8447, val_acc : 61.4900 %, lr : 0.000316\n",
      "epoch : 218, train_loss : 1.8841, val_loss : 1.8472, val_acc : 61.4500 %, lr : 0.000316\n",
      "epoch : 220, train_loss : 1.8660, val_loss : 1.8524, val_acc : 60.5500 %, lr : 0.000316\n",
      "epoch : 222, train_loss : 1.8783, val_loss : 1.8481, val_acc : 61.0800 %, lr : 0.000316\n",
      "epoch : 224, train_loss : 1.8465, val_loss : 1.8471, val_acc : 61.0800 %, lr : 0.000316\n",
      "epoch : 226, train_loss : 1.8515, val_loss : 1.8460, val_acc : 61.4200 %, lr : 0.000316\n",
      "epoch : 228, train_loss : 1.8513, val_loss : 1.8461, val_acc : 61.2300 %, lr : 0.000316\n",
      "epoch : 230, train_loss : 1.8480, val_loss : 1.8464, val_acc : 61.1600 %, lr : 0.000316\n",
      "epoch : 232, train_loss : 1.8617, val_loss : 1.8493, val_acc : 60.9500 %, lr : 0.000316\n",
      "epoch : 234, train_loss : 1.8456, val_loss : 1.8482, val_acc : 60.9200 %, lr : 0.000316\n",
      "epoch : 236, train_loss : 1.8850, val_loss : 1.8459, val_acc : 61.4600 %, lr : 0.000316\n",
      "epoch : 238, train_loss : 1.8519, val_loss : 1.8446, val_acc : 61.3800 %, lr : 0.000316\n",
      "epoch : 240, train_loss : 1.8442, val_loss : 1.8456, val_acc : 61.3600 %, lr : 0.000316\n",
      "epoch : 242, train_loss : 1.8362, val_loss : 1.8445, val_acc : 61.4200 %, lr : 0.000316\n",
      "epoch : 244, train_loss : 1.8083, val_loss : 1.8501, val_acc : 60.7000 %, lr : 0.000316\n",
      "epoch : 246, train_loss : 1.8366, val_loss : 1.8455, val_acc : 61.2800 %, lr : 0.000316\n",
      "epoch : 248, train_loss : 1.8320, val_loss : 1.8435, val_acc : 61.6200 %, lr : 0.000316\n",
      "epoch : 250, train_loss : 1.8402, val_loss : 1.8437, val_acc : 61.4800 %, lr : 0.000237\n",
      "epoch : 252, train_loss : 1.8367, val_loss : 1.8438, val_acc : 61.5200 %, lr : 0.000237\n",
      "epoch : 254, train_loss : 1.8343, val_loss : 1.8448, val_acc : 61.4000 %, lr : 0.000237\n",
      "epoch : 256, train_loss : 1.8455, val_loss : 1.8447, val_acc : 61.4500 %, lr : 0.000237\n",
      "epoch : 258, train_loss : 1.8602, val_loss : 1.8451, val_acc : 61.3500 %, lr : 0.000237\n",
      "epoch : 260, train_loss : 1.8419, val_loss : 1.8452, val_acc : 61.3600 %, lr : 0.000237\n",
      "epoch : 262, train_loss : 1.8519, val_loss : 1.8449, val_acc : 61.4500 %, lr : 0.000237\n",
      "epoch : 264, train_loss : 1.8563, val_loss : 1.8499, val_acc : 60.7500 %, lr : 0.000237\n",
      "epoch : 266, train_loss : 1.8122, val_loss : 1.8435, val_acc : 61.4800 %, lr : 0.000237\n",
      "epoch : 268, train_loss : 1.8380, val_loss : 1.8445, val_acc : 61.5000 %, lr : 0.000237\n",
      "epoch : 270, train_loss : 1.8409, val_loss : 1.8449, val_acc : 61.3800 %, lr : 0.000237\n",
      "epoch : 272, train_loss : 1.8446, val_loss : 1.8442, val_acc : 61.4700 %, lr : 0.000237\n",
      "epoch : 274, train_loss : 1.8287, val_loss : 1.8445, val_acc : 61.4600 %, lr : 0.000237\n",
      "epoch : 276, train_loss : 1.8556, val_loss : 1.8448, val_acc : 61.4700 %, lr : 0.000237\n",
      "epoch : 278, train_loss : 1.8405, val_loss : 1.8473, val_acc : 60.9700 %, lr : 0.000237\n",
      "epoch : 280, train_loss : 1.8448, val_loss : 1.8453, val_acc : 61.1600 %, lr : 0.000237\n",
      "epoch : 282, train_loss : 1.8402, val_loss : 1.8453, val_acc : 61.2500 %, lr : 0.000237\n",
      "epoch : 284, train_loss : 1.8466, val_loss : 1.8448, val_acc : 61.3100 %, lr : 0.000237\n",
      "epoch : 286, train_loss : 1.8634, val_loss : 1.8456, val_acc : 61.3400 %, lr : 0.000237\n",
      "epoch : 288, train_loss : 1.8453, val_loss : 1.8461, val_acc : 61.1500 %, lr : 0.000237\n",
      "epoch : 290, train_loss : 1.7743, val_loss : 1.8477, val_acc : 61.0500 %, lr : 0.000237\n",
      "epoch : 292, train_loss : 1.8273, val_loss : 1.8448, val_acc : 61.2700 %, lr : 0.000237\n",
      "epoch : 294, train_loss : 1.8818, val_loss : 1.8460, val_acc : 61.4000 %, lr : 0.000237\n",
      "epoch : 296, train_loss : 1.8535, val_loss : 1.8446, val_acc : 61.4000 %, lr : 0.000237\n",
      "epoch : 298, train_loss : 1.8434, val_loss : 1.8448, val_acc : 61.3200 %, lr : 0.000237\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "len_val_loader = len(val_loader)\n",
    "len_val_dataset = len(val_dataset)\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_num_corrects = 0.0\n",
    "    \n",
    "    \n",
    "    for image, label in train_loader:\n",
    "        x = image.to(device)\n",
    "        y_ = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        loss = loss_funcion(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            loss = loss_funcion(val_outputs, val_labels)\n",
    "\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_num_corrects += torch.sum(val_preds == val_labels.data)\n",
    "            \n",
    "        \n",
    "    train_loss = loss.cpu().detach().numpy()\n",
    "    \n",
    "    val_epoch_loss = val_loss / len_val_loader\n",
    "    val_epoch_acc = 100 * val_num_corrects.float() / len_val_dataset\n",
    "    \n",
    "    if (i % 2 == 0):\n",
    "        print(f\"epoch : {i}, train_loss : {train_loss:.4f}, val_loss : {val_epoch_loss:.4f}, val_acc : {val_epoch_acc:.4f} %, lr : {optimizer.param_groups[0]['lr']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a62fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : {} 61.48999786376953\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        x = image.to(device)\n",
    "        y_ = label.to(device)\n",
    "\n",
    "        output = model.forward(x)\n",
    "        _,output_index = torch.max(output,1)\n",
    "        total += label.size(0)\n",
    "        correct += (output_index == y_).sum().float()\n",
    "\n",
    "    print(\"Accuracy : {}\", format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df3ee86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './CIFAR/model.pt'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
